#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import tensorflow as tf
from arch.layers import conv2d, conv2d_bn, max_pool, flatten, dense, dense_bn
from arch.layers import Kumar_initializer, truncated_n_seeded, Kumar_initializer_multi
from arch.resnet import residual_layer, residual_block, bottleneck_block
from arch.inception import grid_module_v1
from arch.inception import grid_module_v2, factorized_grid_module_v2
from arch.inception import bn_grid_module_v2
from arch.inception import filterbank_grid_module_v2, reduction_module_v2
from arch.inception import auxiliary_classifier_v3
from arch.inception import filterbank_grid_module_v4, reduction_module_v4
from arch import resnext


def mnist_sequential_dbn2d1(x, drop_rate=0.5):
    """Creates sequential neural network for MNIST. The network contains 2 
        batch-normalized fully-connected dense layers. Dropout layers are used 
        for regularization. The output  probabilities are generated by one 
        dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    flat = flatten(x, name="flatten")
    layers.append(("flatten", flat))

    fc1 = dense_bn(flat, n_units=256, is_training=training, name="fc1")
    layers.append(("fc1", fc1))
    
    dropout1 = tf.layers.dropout(fc1, rate=drop_rate, training=training, seed=42, name="dropout1")
    layers.append(("dropout1", dropout1))

    fc2 = dense_bn(dropout1, n_units=64, is_training=training, name="fc2")
    layers.append(("fc2", fc2))
    
    dropout2 = tf.layers.dropout(fc2, rate=drop_rate, training=training, seed=42, name="dropout2")
    layers.append(("dropout2", dropout2))
    
    fc3 = dense(dropout2, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None),
                name="fc3")
    layers.append(("fc3", fc3))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


def mnist_sequential_c2d2(x, drop_rate=0.5):
    """Creates sequential convolutional neural network for MNIST. The network
        uses 2 convolutional+pooling layers to create the representation part
        of the network, and 1 fully-connected dense layer to create the
        classifier part. Dropout layer is used for regularization. The output 
        probabilities are generated by one dense layer followed by a softmax 
        function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d(x, size=5, n_filters=32,
                   kernel_init=Kumar_initializer(mode="FAN_AVG"),
                   name="conv1")
    layers.append(("conv1", conv1))
            
    pool1 = max_pool(conv1, name="pool1")
    layers.append(("pool1", pool1))

    conv2 = conv2d(pool1, size=5, n_filters=64,
                   kernel_init=Kumar_initializer(mode="FAN_IN"),
                   name="conv2")
    layers.append(("conv2", conv2))
        
    pool2 = max_pool(conv2, name="pool2")
    layers.append(("pool2", pool2))

    flat = flatten(pool2, name="flatten")
    layers.append(("flatten", flat))

    fc1 = dense(flat, n_units=1024,
                kernel_init=Kumar_initializer(mode="FAN_IN"),
                name="fc1")
    layers.append(("fc1", fc1))
    
    dropout1 = tf.layers.dropout(fc1, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))
    
    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


def mnist_sequential_c2dbn1d1(x, drop_rate=0.5):
    """Creates sequential convolutional neural network for MNIST. The network
        uses 2 convolutional+pooling layers to create the representation part
        of the network, and 1 fully-connected dense layer to create the
        classifier part. Batch normalization is applied in the dense layer.
        Dropout layer is used for regularization. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d(x, size=5, n_filters=32,
                   kernel_init=Kumar_initializer(mode="FAN_AVG"),
                   name="conv1")
    layers.append(("conv1", conv1))
            
    pool1 = max_pool(conv1, name="pool1")
    layers.append(("pool1", pool1))

    conv2 = conv2d(pool1, size=5, n_filters=64,
                   kernel_init=Kumar_initializer(mode="FAN_IN"),
                   name="conv2")
    layers.append(("conv2", conv2))
        
    pool2 = max_pool(conv2, name="pool2")
    layers.append(("pool2", pool2))

    flat = flatten(pool2, name="flatten")
    layers.append(("flatten", flat))

    fc1 = dense_bn(flat, n_units=1024, is_training=training,
                   kernel_init=Kumar_initializer(mode="FAN_IN"),
                   name="fc1")
    layers.append(("fc1", fc1))
    
    dropout1 = tf.layers.dropout(fc1, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))
    
    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables
    

def mnist_resnet_cbn1r3d1(x):
    """Creates residual neural network for MNIST. The network
        uses 1 batch normalized convolutional and 3 residual layers to create 
        the representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d_bn(x, size=3, n_filters=16,
                      kernel_init=Kumar_initializer(mode="FAN_AVG"),
                      name="initial_conv")
    layers.append(("initial_conv", conv1))
            
    res1 = residual_layer(conv1, n_filters=16, n_blocks=2, stride=1,
                          is_training=training, name="residual1")
    layers.append(("residual1", res1))

    res2 = residual_layer(res1, n_filters=32, n_blocks=2, stride=2,
                          is_training=training, name="residual2")
    layers.append(("residual2", res2))

    res3 = residual_layer(res2, n_filters=64, n_blocks=2, stride=2,
                          is_training=training, name="residual3")
    layers.append(("residual3", res3))

    pool1 = tf.reduce_mean(res3, [1,2]) # global average pooling
    layers.append(("pool", pool1))
    
    fc2 = dense(pool1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


def cifar10_sequential_cn2c2cnd3(x, drop_rate=0.25):
    """Creates sequential convolutional neural network for CIFAR10. The network
        uses 2 locally normalized convolutional+pooling, followed by 
        2 convolutional and 1 locally normalized convolutional+pooling layers
        to create the representation part of the network, and 2 fully-connected 
        dense layer to create the classifier part. Dropout layer is used for 
        regularization. The output  probabilities are generated by one dense 
        layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d(x, size=5, n_filters=64, kernel_init=truncated_n_seeded(stddev=5e-2), name="conv1")
    layers.append(("conv1", conv1))
    
    norm1 = tf.nn.lrn(conv1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')
    layers.append(("norm1", norm1))
            
    pool1 = max_pool(norm1, size=3, stride=2, name="pool1")
    layers.append(("pool1", pool1))

    conv2 = conv2d(pool1, size=5, n_filters=64, kernel_init=truncated_n_seeded(stddev=5e-2), name="conv2")
    layers.append(("conv2", conv2))
            
    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')
    layers.append(("norm2", norm2))

    pool2 = max_pool(norm2, size=3, stride=2, name="pool2")
    layers.append(("pool2", pool2))

    conv3 = conv2d(pool2, size=3, n_filters=128, kernel_init=truncated_n_seeded(stddev=5e-2), name="conv3")
    layers.append(("conv3", conv3))

    conv4 = conv2d(conv3, size=3, n_filters=128, kernel_init=truncated_n_seeded(stddev=5e-2), name="conv4")
    layers.append(("conv4", conv4))
            
    conv5 = conv2d(conv3, size=3, n_filters=128, kernel_init=truncated_n_seeded(stddev=5e-2), name="conv5")
    layers.append(("conv5", conv5))

    norm5 = tf.nn.lrn(conv5, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm5')
    layers.append(("norm5", norm5))

    pool5 = max_pool(norm5, size=3, stride=2, name="pool5")
    layers.append(("pool5", pool5))


    flat = flatten(pool2, name="flatten")
    layers.append(("flatten", flat))

    fc1 = dense(flat, n_units=384,
                kernel_init=truncated_n_seeded(stddev=0.04),
                bias_init=tf.constant_initializer(0.1),
                name="fc1")
    layers.append(("fc1", fc1))
    
    dropout1 = tf.layers.dropout(fc1, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=192,
                kernel_init=truncated_n_seeded(stddev=0.04),
                bias_init=tf.constant_initializer(0.1),
                name="fc2")
    layers.append(("fc2", fc2))

    fc3 = dense(fc2, n_units=10, activation=None,
                kernel_init=truncated_n_seeded(stddev=1 / 192.0),
                name="fc3")
    layers.append(("fc3", fc3))
    
    prob = tf.nn.softmax(fc3, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


def cifar10_sequential_c5d3(x, drop_rate=0.25):
    """Creates sequential convolutional neural network for CIFAR10. The network
        uses 5 convolutional layers and 3 poolings to create the representation 
        part of the network, and 2 fully-connected  dense layer to create the 
        classifier part. Dropout layer is used for regularization. The output 
        probabilities are generated by one dense layer followed by a softmax 
        function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d(x, size=5, n_filters=64,
                   kernel_init=Kumar_initializer(mode="FAN_AVG"),
                   name="conv1")
    layers.append(("conv1", conv1))
                
    pool1 = max_pool(conv1, name="pool1")
    layers.append(("pool1", pool1))

    conv2 = conv2d(pool1, size=5, n_filters=64,
                   kernel_init=Kumar_initializer(mode="FAN_IN"),
                   name="conv2")
    layers.append(("conv2", conv2))
            
    pool2 = max_pool(conv2, name="pool2")
    layers.append(("pool2", pool2))

    conv3 = conv2d(pool2, size=3, n_filters=128,
                   kernel_init=Kumar_initializer(mode="FAN_IN"),
                   name="conv3")
    layers.append(("conv3", conv3))

    conv4 = conv2d(conv3, size=3, n_filters=128,
                   kernel_init=Kumar_initializer(mode="FAN_IN"),
                   name="conv4")
    layers.append(("conv4", conv4))
            
    conv5 = conv2d(conv3, size=3, n_filters=128,
                   kernel_init=Kumar_initializer(mode="FAN_IN"),
                   name="conv5")
    layers.append(("conv5", conv5))

    pool5 = max_pool(conv5, name="pool5")
    layers.append(("pool5", pool5))

    # flatten
    flat = flatten(pool2, name="flatten")
    layers.append(("flatten", flat))

    fc1 = dense(flat, n_units=384,
                kernel_init=Kumar_initializer(mode="FAN_IN"),
                name="fc1")
    layers.append(("fc1", fc1))
    
    dropout1 = tf.layers.dropout(fc1, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=192,
                kernel_init=Kumar_initializer(mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    fc3 = dense(fc2, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc3")
    layers.append(("fc3", fc3))
    
    prob = tf.nn.softmax(fc3, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


def cifar10_sequential_cbn3dbn3(x, drop_rate=0.25):
    """Creates sequential convolutional neural network for CIFAR10. The network
        uses 5 convolutional layers and 3 poolings, followed by  to create the 
        representation part of the network, and 2 fully-connected  dense layer 
        to create the classifier part. Dropout layer is used for regularization.
        The output  probabilities are generated by one dense  layer followed by 
        a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d_bn(x, size=5, n_filters=64, name="conv1")
    layers.append(("conv1", conv1))
                
    pool1 = max_pool(conv1, name="pool1")
    layers.append(("pool1", pool1))

    conv2 = conv2d_bn(pool1, size=5, n_filters=64,
                      kernel_init=Kumar_initializer(mode="FAN_IN"),
                      name="conv2")
    layers.append(("conv2", conv2))
            
    pool2 = max_pool(conv2, name="pool2")
    layers.append(("pool2", pool2))

    conv3 = conv2d_bn(pool2, size=3, n_filters=128,
                      kernel_init=Kumar_initializer(mode="FAN_IN"),
                      name="conv3")
    layers.append(("conv3", conv3))

    pool3 = max_pool(conv3, name="pool3")
    layers.append(("pool3", pool3))

    flat = flatten(pool3, name="flatten")
    layers.append(("flatten", flat))

    fc1 = dense_bn(flat, n_units=384, name="fc1")
    layers.append(("fc1", fc1))
    
    dropout1 = tf.layers.dropout(fc1, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense_bn(dropout1, n_units=192, name="fc2")
    layers.append(("fc2", fc2))
    
    fc3 = dense(fc2, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None),
                name="fc3")
    layers.append(("fc3", fc3))
    
    prob = tf.nn.softmax(fc3, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


def cifar10_resnet_bottleneck_20(x):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d_bn(x, size=3, n_filters=64,
                      kernel_init=Kumar_initializer(mode="FAN_AVG"),
                      name="initial_conv")
    layers.append(("initial_conv", conv1))
            
    res1 = residual_layer(conv1, n_filters=128, n_blocks=3, stride=1,
                          block_function = residual_block,
                          is_training=training, name="residual1")
    layers.append(("residual1", res1))

    res2 = residual_layer(res1, n_filters=[64,256], n_blocks=4, stride=2,
                          block_function = bottleneck_block,
                          is_training=training, name="residual2")
    layers.append(("residual2", res2))

    res3 = residual_layer(res2, n_filters=[128,512], n_blocks=2, stride=2,
                          block_function = bottleneck_block,
                          is_training=training, name="residual3")
    layers.append(("residual3", res3))

    pool1 = tf.reduce_mean(res3, [1,2]) # global average pooling
    layers.append(("pool", pool1))
    
    fc2 = dense(pool1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


def cifar10_resnet_20(x):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d_bn(x, size=3, n_filters=64,
                      kernel_init=Kumar_initializer(mode="FAN_AVG"),
                      name="initial_conv")
    layers.append(("initial_conv", conv1))
            
    res1 = residual_layer(conv1, n_filters=128, n_blocks=3, stride=1,
                          is_training=training, name="residual1")
    layers.append(("residual1", res1))

    res2 = residual_layer(res1, n_filters=256, n_blocks=4, stride=2,
                          is_training=training, name="residual2")
    layers.append(("residual2", res2))

    res3 = residual_layer(res2, n_filters=512, n_blocks=2, stride=2,
                          is_training=training, name="residual3")
    layers.append(("residual3", res3))

    pool1 = tf.reduce_mean(res3, [1,2]) # global average pooling
    layers.append(("pool", pool1))
    
    fc2 = dense(pool1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables



def cifar10_resnext_20(x):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d_bn(x, size=3, n_filters=64,
                      kernel_init=Kumar_initializer(mode="FAN_AVG"),
                      name="initial_conv")
    layers.append(("initial_conv", conv1))
            
    res1 = resnext.residual_layer(
            conv1,
            n_filters_reduce=64, n_filters=128, split_depth=4,
            n_blocks=3, stride=1,
            is_training=training, name="residual1")
    layers.append(("residual1", res1))

    res2 = resnext.residual_layer(
            res1,
            n_filters_reduce=128, n_filters=256, split_depth=4,
            n_blocks=4, stride=2,
            is_training=training, name="residual2")
    layers.append(("residual2", res2))

    res3 = resnext.residual_layer(
            res2,
            n_filters_reduce=256, n_filters=512, split_depth=4,
            n_blocks=2, stride=2,
            is_training=training, name="residual3")
    layers.append(("residual3", res3))

    pool1 = tf.reduce_mean(res3, [1,2]) # global average pooling
    layers.append(("pool", pool1))
    
    fc2 = dense(pool1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables



#https://arxiv.org/pdf/1409.4842v1.pdf
def cifar10_inception_v1(x, drop_rate=0.5):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    grid1 = grid_module_v1(
                    x,
                    n_filters_1x1 = 64,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 128,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 32,
                    n_filters_pool = 32,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_1"            
                    )
    layers.append(("grid1", grid1))
    
    pool1 = max_pool(grid1, size=3, stride=2, name="pool1")
    layers.append(("pool1", pool1))
            
    grid2 = grid_module_v1(
                    pool1,
                    n_filters_1x1 = 192,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 208,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 48,
                    n_filters_pool = 64,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_2" 
                    )
    layers.append(("grid2", grid2))
    
    pool2 = max_pool(grid2, size=3, stride=2, name="pool2")
    layers.append(("pool2", pool2))

    grid3 = grid_module_v1(
                    pool1,
                    n_filters_1x1 = 256,
                    n_reduce_3x3 = 160,
                    n_filters_3x3 = 320,
                    n_reduce_5x5 = 32,
                    n_filters_5x5 = 128,
                    n_filters_pool = 128,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_3" 
                    )
    layers.append(("grid3", grid3))

    pool3 = tf.reduce_mean(grid3, [1,2]) # global average pooling 8x8
    layers.append(("pool3", pool3))
             
    dropout1 = tf.layers.dropout(pool3, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


#https://arxiv.org/pdf/1502.03167.pdf
def cifar10_bn_inception_v1(x, drop_rate=0.5):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    grid1 = bn_grid_module_v2(
                    x,
                    n_filters_1x1 = 64,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 128,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 32,
                    n_filters_pool = 32,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_1"            
                    )
    layers.append(("grid1", grid1))
    
    pool1 = max_pool(grid1, size=3, stride=2, name="pool1")
    layers.append(("pool1", pool1))
            
    grid2 = bn_grid_module_v2(
                    pool1,
                    n_filters_1x1 = 192,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 208,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 48,
                    n_filters_pool = 64,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_2" 
                    )
    layers.append(("grid2", grid2))
    
    pool2 = max_pool(grid2, size=3, stride=2, name="pool2")
    layers.append(("pool2", pool2))

    grid3 = bn_grid_module_v2(
                    pool1,
                    n_filters_1x1 = 256,
                    n_reduce_3x3 = 160,
                    n_filters_3x3 = 320,
                    n_reduce_5x5 = 32,
                    n_filters_5x5 = 128,
                    n_filters_pool = 128,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_3" 
                    )
    layers.append(("grid3", grid3))

    pool3 = tf.reduce_mean(grid3, [1,2]) # global average pooling 8x8
    layers.append(("pool3", pool3))
             
    dropout1 = tf.layers.dropout(pool3, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables

#https://arxiv.org/pdf/1512.00567.pdf
def cifar10_inception_v2(x, drop_rate=0.5):
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    grid1 = grid_module_v2(
                    x,
                    n_filters_1x1 = 64,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 128,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 32,
                    n_filters_pool = 32,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_1"            
                    )
    layers.append(("grid1", grid1))
    
    reduction1 = reduction_module_v2(
                    grid1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_1"
                    )
    layers.append(("reduction1", reduction1))
            
    grid2 = factorized_grid_module_v2(
                    reduction1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_2" 
                    )
    layers.append(("grid2", grid2))
    
    reduction2 = reduction_module_v2(
                    grid2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_2"
                    )
    layers.append(("reduction2", reduction2))

    grid3 = filterbank_grid_module_v2(
                    reduction2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_3" 
                    )
    layers.append(("grid3", grid3))

    pool3 = tf.reduce_mean(grid3, [1,2]) # global average pooling 8x8
    layers.append(("pool3", pool3))
             
    dropout1 = tf.layers.dropout(pool3, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables

    
#https://arxiv.org/pdf/1512.00567.pdf
#inception2 + label smoothing + factorized 7x7 + BN-aux
def cifar10_inception_v3(x, drop_rate=0.5):    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    grid1 = grid_module_v2(
                    x,
                    n_filters_1x1 = 64,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 128,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 32,
                    n_filters_pool = 32,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_1"            
                    )
    layers.append(("grid1", grid1))
    
    reduction1 = reduction_module_v2(
                    grid1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_1"
                    )
    layers.append(("reduction1", reduction1))
            
    grid2 = factorized_grid_module_v2(
                    reduction1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_2" 
                    )
    layers.append(("grid2", grid2))
    
    aux = auxiliary_classifier_v3(
            grid2,
            mode = "FAN_AVG",
            name = "auxiliary_classifier")
    layers.append(("aux", aux))
    
    aux_dropout = tf.layers.dropout(aux, rate=drop_rate, training=training, seed=42, name="aux_dropout")
    layers.append(("aux_dropout", aux_dropout))
    
    aux_fc = dense(aux_dropout, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="aux_fc")
    layers.append(("aux_fc", aux_fc))
    
    aux_prob = tf.nn.softmax(aux_fc, name="aux_prob")
    layers.append(("aux_prob", aux_prob))    
    
    reduction2 = reduction_module_v2(
                    grid2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_2"
                    )
    layers.append(("reduction2", reduction2))

    grid3 = filterbank_grid_module_v2(
                    reduction2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_3" 
                    )
    layers.append(("grid3", grid3))

    pool3 = tf.reduce_mean(grid3, [1,2]) # global average pooling 8x8
    layers.append(("pool3", pool3))
             
    dropout1 = tf.layers.dropout(pool3, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


# https://arxiv.org/pdf/1602.07261.pdf
def cifar10_inception_v4(x, drop_rate=0.5):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    grid1 = grid_module_v2(
                    x,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_1"
                    )
    layers.append(("grid1", grid1))
    
    reduction1 = reduction_module_v2(
                    grid1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_1"
                    )
    layers.append(("reduction1", reduction1))
            
    grid2 = factorized_grid_module_v2(
                    reduction1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_2" 
                    )
    layers.append(("grid2", grid2))
    
    reduction2 = reduction_module_v4(
                    grid2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_2"
                    )
    layers.append(("reduction2", reduction2))    
    
    grid3 = filterbank_grid_module_v4(
                    reduction2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_3" 
                    )
    layers.append(("grid3", grid3))
    
    pool1 = tf.reduce_mean(grid3, [1,2]) # global average pooling 8x8
    layers.append(("pool1", pool1))
             
    dropout1 = tf.layers.dropout(pool1, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables

