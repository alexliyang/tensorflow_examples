#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from functools import partial
import tensorflow as tf
from arch.layers import conv2d, conv2d_bn, max_pool, flatten, dense, dense_bn
from arch.layers import Kumar_initializer, truncated_n_seeded, Kumar_initializer_multi
from arch.inception import grid_module_v1
from arch.inception import grid_module_v2, factorized_grid_module_v2
from arch.inception import bn_grid_module_v2
from arch.inception import filterbank_grid_module_v2, reduction_module_v2
from arch.inception import auxiliary_classifier_v3
from arch.inception import filterbank_grid_module_v4, reduction_module_v4
from arch import resnet
from arch import resnext
from arch import xception
from arch import densenet
from arch import senet
from arch import mobilenet
from arch import shufflenet


def mnist_sequential_dbn2d1(x, drop_rate=0.5):
    """Creates sequential neural network for MNIST. The network contains 2 
        batch-normalized fully-connected dense layers. Dropout layers are used 
        for regularization. The output  probabilities are generated by one 
        dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    flat = flatten(x, name="flatten")
    layers.append(("flatten", flat))

    fc1 = dense_bn(flat, n_units=256, is_training=training, name="fc1")
    layers.append(("fc1", fc1))
    
    dropout1 = tf.layers.dropout(fc1, rate=drop_rate, training=training, seed=42, name="dropout1")
    layers.append(("dropout1", dropout1))

    fc2 = dense_bn(dropout1, n_units=64, is_training=training, name="fc2")
    layers.append(("fc2", fc2))
    
    dropout2 = tf.layers.dropout(fc2, rate=drop_rate, training=training, seed=42, name="dropout2")
    layers.append(("dropout2", dropout2))
    
    fc3 = dense(dropout2, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None),
                name="fc3")
    layers.append(("fc3", fc3))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


def mnist_sequential_c2d2(x, drop_rate=0.5):
    """Creates sequential convolutional neural network for MNIST. The network
        uses 2 convolutional+pooling layers to create the representation part
        of the network, and 1 fully-connected dense layer to create the
        classifier part. Dropout layer is used for regularization. The output 
        probabilities are generated by one dense layer followed by a softmax 
        function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d(x, size=5, n_filters=32,
                   kernel_init=Kumar_initializer(mode="FAN_AVG"),
                   name="conv1")
    layers.append(("conv1", conv1))
            
    pool1 = max_pool(conv1, name="pool1")
    layers.append(("pool1", pool1))

    conv2 = conv2d(pool1, size=5, n_filters=64,
                   kernel_init=Kumar_initializer(mode="FAN_IN"),
                   name="conv2")
    layers.append(("conv2", conv2))
        
    pool2 = max_pool(conv2, name="pool2")
    layers.append(("pool2", pool2))

    flat = flatten(pool2, name="flatten")
    layers.append(("flatten", flat))

    fc1 = dense(flat, n_units=1024,
                kernel_init=Kumar_initializer(mode="FAN_IN"),
                name="fc1")
    layers.append(("fc1", fc1))
    
    dropout1 = tf.layers.dropout(fc1, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))
    
    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


def mnist_sequential_c2dbn1d1(x, drop_rate=0.5):
    """Creates sequential convolutional neural network for MNIST. The network
        uses 2 convolutional+pooling layers to create the representation part
        of the network, and 1 fully-connected dense layer to create the
        classifier part. Batch normalization is applied in the dense layer.
        Dropout layer is used for regularization. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d(x, size=5, n_filters=32,
                   kernel_init=Kumar_initializer(mode="FAN_AVG"),
                   name="conv1")
    layers.append(("conv1", conv1))
            
    pool1 = max_pool(conv1, name="pool1")
    layers.append(("pool1", pool1))

    conv2 = conv2d(pool1, size=5, n_filters=64,
                   kernel_init=Kumar_initializer(mode="FAN_IN"),
                   name="conv2")
    layers.append(("conv2", conv2))
        
    pool2 = max_pool(conv2, name="pool2")
    layers.append(("pool2", pool2))

    flat = flatten(pool2, name="flatten")
    layers.append(("flatten", flat))

    fc1 = dense_bn(flat, n_units=1024, is_training=training,
                   kernel_init=Kumar_initializer(mode="FAN_IN"),
                   name="fc1")
    layers.append(("fc1", fc1))
    
    dropout1 = tf.layers.dropout(fc1, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))
    
    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables
    

def mnist_resnet_cbn1r3d1(x):
    """Creates residual neural network for MNIST. The network
        uses 1 batch normalized convolutional and 3 residual layers to create 
        the representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv1 = conv2d_bn(x, size=3, n_filters=16,
                      kernel_init=Kumar_initializer(mode="FAN_AVG"),
                      name="initial_conv")
    layers.append(("initial_conv", conv1))
            
    res1 = resnet.residual_layer(
            conv1, n_filters=16, n_blocks=2, stride=1,
            is_training=training, name="residual1")
    layers.append(("residual1", res1))

    res2 = resnet.residual_layer(
            res1, n_filters=32, n_blocks=2, stride=2,
            is_training=training, name="residual2")
    layers.append(("residual2", res2))

    res3 = resnet.residual_layer(
            res2, n_filters=64, n_blocks=2, stride=2,
            is_training=training, name="residual3")
    layers.append(("residual3", res3))

    pool1 = tf.reduce_mean(res3, [1,2]) # global average pooling
    layers.append(("pool", pool1))
    
    fc2 = dense(pool1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables













#https://arxiv.org/pdf/1409.4842v1.pdf
def cifar10_inception_v1(x, drop_rate=0.5):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    grid1 = grid_module_v1(
                    x,
                    n_filters_1x1 = 64,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 128,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 32,
                    n_filters_pool = 32,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_1"            
                    )
    layers.append(("grid1", grid1))
    
    pool1 = max_pool(grid1, size=3, stride=2, name="pool1")
    layers.append(("pool1", pool1))
            
    grid2 = grid_module_v1(
                    pool1,
                    n_filters_1x1 = 192,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 208,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 48,
                    n_filters_pool = 64,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_2" 
                    )
    layers.append(("grid2", grid2))
    
    pool2 = max_pool(grid2, size=3, stride=2, name="pool2")
    layers.append(("pool2", pool2))

    grid3 = grid_module_v1(
                    pool1,
                    n_filters_1x1 = 256,
                    n_reduce_3x3 = 160,
                    n_filters_3x3 = 320,
                    n_reduce_5x5 = 32,
                    n_filters_5x5 = 128,
                    n_filters_pool = 128,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_3" 
                    )
    layers.append(("grid3", grid3))

    pool3 = tf.reduce_mean(grid3, [1,2]) # global average pooling 8x8
    layers.append(("pool3", pool3))
             
    dropout1 = tf.layers.dropout(pool3, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


#https://arxiv.org/pdf/1502.03167.pdf
def cifar10_bn_inception_v1(x, drop_rate=0.5):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    grid1 = bn_grid_module_v2(
                    x,
                    n_filters_1x1 = 64,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 128,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 32,
                    n_filters_pool = 32,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_1"            
                    )
    layers.append(("grid1", grid1))
    
    pool1 = max_pool(grid1, size=3, stride=2, name="pool1")
    layers.append(("pool1", pool1))
            
    grid2 = bn_grid_module_v2(
                    pool1,
                    n_filters_1x1 = 192,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 208,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 48,
                    n_filters_pool = 64,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_2" 
                    )
    layers.append(("grid2", grid2))
    
    pool2 = max_pool(grid2, size=3, stride=2, name="pool2")
    layers.append(("pool2", pool2))

    grid3 = bn_grid_module_v2(
                    pool1,
                    n_filters_1x1 = 256,
                    n_reduce_3x3 = 160,
                    n_filters_3x3 = 320,
                    n_reduce_5x5 = 32,
                    n_filters_5x5 = 128,
                    n_filters_pool = 128,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_3" 
                    )
    layers.append(("grid3", grid3))

    pool3 = tf.reduce_mean(grid3, [1,2]) # global average pooling 8x8
    layers.append(("pool3", pool3))
             
    dropout1 = tf.layers.dropout(pool3, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables

#https://arxiv.org/pdf/1512.00567.pdf
def cifar10_inception_v2(x, drop_rate=0.5):
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    grid1 = grid_module_v2(
                    x,
                    n_filters_1x1 = 64,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 128,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 32,
                    n_filters_pool = 32,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_1"            
                    )
    layers.append(("grid1", grid1))
    
    reduction1 = reduction_module_v2(
                    grid1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_1"
                    )
    layers.append(("reduction1", reduction1))
            
    grid2 = factorized_grid_module_v2(
                    reduction1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_2" 
                    )
    layers.append(("grid2", grid2))
    
    reduction2 = reduction_module_v2(
                    grid2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_2"
                    )
    layers.append(("reduction2", reduction2))

    grid3 = filterbank_grid_module_v2(
                    reduction2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_3" 
                    )
    layers.append(("grid3", grid3))

    pool3 = tf.reduce_mean(grid3, [1,2]) # global average pooling 8x8
    layers.append(("pool3", pool3))
             
    dropout1 = tf.layers.dropout(pool3, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables

    
#https://arxiv.org/pdf/1512.00567.pdf
#inception2 + label smoothing + factorized 7x7 + BN-aux
def cifar10_inception_v3(x, drop_rate=0.5):    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    grid1 = grid_module_v2(
                    x,
                    n_filters_1x1 = 64,
                    n_reduce_3x3 = 96,
                    n_filters_3x3 = 128,
                    n_reduce_5x5 = 16,
                    n_filters_5x5 = 32,
                    n_filters_pool = 32,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "inception_grid_1"            
                    )
    layers.append(("grid1", grid1))
    
    reduction1 = reduction_module_v2(
                    grid1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_1"
                    )
    layers.append(("reduction1", reduction1))
            
    grid2 = factorized_grid_module_v2(
                    reduction1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_2" 
                    )
    layers.append(("grid2", grid2))
    
    aux = auxiliary_classifier_v3(
            grid2,
            mode = "FAN_AVG",
            name = "auxiliary_classifier")
    layers.append(("aux", aux))
    
    aux_dropout = tf.layers.dropout(aux, rate=drop_rate, training=training, seed=42, name="aux_dropout")
    layers.append(("aux_dropout", aux_dropout))
    
    aux_fc = dense(aux_dropout, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="aux_fc")
    layers.append(("aux_fc", aux_fc))
    
    aux_prob = tf.nn.softmax(aux_fc, name="aux_prob")
    layers.append(("aux_prob", aux_prob))    
    
    reduction2 = reduction_module_v2(
                    grid2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_2"
                    )
    layers.append(("reduction2", reduction2))

    grid3 = filterbank_grid_module_v2(
                    reduction2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_3" 
                    )
    layers.append(("grid3", grid3))

    pool3 = tf.reduce_mean(grid3, [1,2]) # global average pooling 8x8
    layers.append(("pool3", pool3))
             
    dropout1 = tf.layers.dropout(pool3, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


# https://arxiv.org/pdf/1602.07261.pdf
def cifar10_inception_v4(x, drop_rate=0.5):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    grid1 = grid_module_v2(
                    x,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_1"
                    )
    layers.append(("grid1", grid1))
    
    reduction1 = reduction_module_v2(
                    grid1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_1"
                    )
    layers.append(("reduction1", reduction1))
            
    grid2 = factorized_grid_module_v2(
                    reduction1,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_2" 
                    )
    layers.append(("grid2", grid2))
    
    reduction2 = reduction_module_v4(
                    grid2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "reduction_2"
                    )
    layers.append(("reduction2", reduction2))    
    
    grid3 = filterbank_grid_module_v4(
                    reduction2,
                    is_training = training,
                    mode = "FAN_AVG",
                    name = "grid_3" 
                    )
    layers.append(("grid3", grid3))
    
    pool1 = tf.reduce_mean(grid3, [1,2]) # global average pooling 8x8
    layers.append(("pool1", pool1))
             
    dropout1 = tf.layers.dropout(pool1, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout1", dropout1))

    fc2 = dense(dropout1, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables


def cifar10_xception(x, drop_rate=0.5):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))


    conv1 = conv2d_bn(x, size=3, n_filters=64,
                      kernel_init=Kumar_initializer(mode="FAN_AVG"),
                      name="initial_conv")
    layers.append(("initial_conv", conv1))
    
    entry = xception.entry_module(
            conv1,
            n_filters=[128],
            is_training=training,
            kernel_init=Kumar_initializer(mode="FAN_AVG"),
            name="entry")
    layers.append(("entry", entry))
    
    mid = xception.middle_module(
            entry,
            n_filters=128,
            n_repeat=4,
            is_training=training,
            name="middle",
            )
    layers.append(("middle", mid))

    exits = xception.exit_module(
            mid,
            n_filters_1=[128, 256],
            n_filters_2=[384, 512],
            is_training=training,
            name="exit")
    layers.append(("exit", exits))
    
    pool = tf.reduce_mean(exits, [1,2]) # global average pooling 8x8
    layers.append(("pool", pool))
             
    dropout = tf.layers.dropout(pool, rate=drop_rate, training=training, seed=42, name="dropout")
    layers.append(("dropout", dropout))

    fc2 = dense(dropout, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables




def cifar10_mobilenet(x):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv = conv2d_bn(
            x, size=3, n_filters=32,
            kernel_init=Kumar_initializer(mode="FAN_AVG"),
            is_training=training,
            name="initial_conv")
    layers.append(("initial_conv", conv))
    
    mblock1 = mobilenet.mobilenet_block(
            conv, n_filters=64, stride=1,
            kernel_init = Kumar_initializer(mode="FAN_AVG"),
            is_training=training,
            name="mobilenet_block1"
            )            
    layers.append(("mobilenet_block1", mblock1))

    mblock2 = mobilenet.mobilenet_block(
            mblock1, n_filters=128, stride=2,
            kernel_init = Kumar_initializer(mode="FAN_AVG"),
            is_training=training,
            name="mobilenet_block2"
            )            
    layers.append(("mobilenet_block2", mblock2))

    mblock3 = mobilenet.mobilenet_block(
            mblock2, n_filters=256, stride=2,
            kernel_init = Kumar_initializer(mode="FAN_AVG"),
            is_training=training,
            name="mobilenet_block3"
            )            
    layers.append(("mobilenet_block3", mblock3))

    mblock4 = mobilenet.mobilenet_block(
            mblock3, n_filters=512, stride=2,
            kernel_init = Kumar_initializer(mode="FAN_AVG"),
            is_training=training,
            name="mobilenet_block4"
            )            
    layers.append(("mobilenet_block4", mblock4))

    pool = tf.reduce_mean(mblock4, [1,2]) # global average pooling
    layers.append(("pool", pool))
    
    fc2 = dense(pool, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables



def cifar10_shufflenet(x):
    """Creates residual neural network for CIFAR10. The network uses 1 batch 
        normalized convolutional and 3 residual layers to create the 
        representation part of the network. The output probabilities are 
        generated by one dense layer followed by a softmax function.
    Args:
        x: A tensor representing the input.
    Returns:
        A tuple containing the layers of the network graph and additional
        placeholders if any. Layers are represented as list of named tuples.
    """    
    
    layers = []
    variables = []

    training = tf.placeholder(tf.bool, name="training")
    variables.append(("training", training))

    conv = conv2d_bn(
            x, size=3, n_filters=24,
            is_training=training,
            kernel_init=Kumar_initializer(mode="FAN_AVG"),
            name="initial_conv")
    layers.append(("initial_conv", conv))
    
    slayer1 = shufflenet.shufflenet_layer(
            conv, n_filters = 200,
            n_repeat = 3, n_groups = 2,
            is_training = training,
            kernel_init = Kumar_initializer(mode="FAN_AVG"),
            name = "shufflenet_layer1")
    layers.append(("shufflenet_layer1", slayer1))
            
    slayer2 = shufflenet.shufflenet_layer(
            slayer1, n_filters = 400,
            n_repeat = 7, n_groups = 2,
            is_training = training,
            kernel_init = Kumar_initializer(mode="FAN_AVG"),
            name = "shufflenet_layer2")
    layers.append(("shufflenet_layer2", slayer2))

    slayer3 = shufflenet.shufflenet_layer(
            slayer2, n_filters = 800,
            n_repeat = 3, n_groups = 2,
            is_training = training,
            kernel_init = Kumar_initializer(mode="FAN_AVG"),
            name = "shufflenet_layer3")
    layers.append(("shufflenet_layer3", slayer3))

    pool = tf.reduce_mean(slayer3, [1,2])
    layers.append(("pool", pool))
    
    fc2 = dense(pool, n_units=10, activation=None,
                kernel_init=Kumar_initializer(activation=None, mode="FAN_IN"),
                name="fc2")
    layers.append(("fc2", fc2))
    
    prob = tf.nn.softmax(fc2, name="prob")
    layers.append(("prob", prob))
    
    return layers, variables
